{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Manav\\Personal_Projects\\Source-Code_Analysis_Using_Gen_AI\\llmapp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from git import Repo\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = \"AIzaSyDlHdr-HiAALsowWrprOm4ofc3QYFZZ7-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyDlHdr-HiAALsowWrprOm4ofc3QYFZZ7-8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/aqa\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "for model in genai.list_models():\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloning a GIT Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo 'd:\\\\Manav\\\\Personal_Projects\\\\Source-Code_Analysis_Using_Gen_AI\\\\research\\\\test_repo\\\\.git'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a test folder for saving git repo for testing\n",
    "!mkdir test_repo\n",
    "\n",
    "# Cloning a repo \n",
    "Repo.clone_from(\"https://github.com/Manav446/Sport_Classification_Using_Images_Project.git\", to_path=\"test_repo/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path + \"src/cnnClassifier\", \n",
    "    glob=\"**/*\", suffixes=[\".py\"], \n",
    "    parser=LanguageParser(\n",
    "        language=Language.PYTHON, \n",
    "        parser_threshold=500\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader2 = GenericLoader.from_filesystem(\n",
    "    repo_path, \n",
    "    glob=\"**/*\", suffixes=[\".py\"], \n",
    "    parser=LanguageParser(\n",
    "        language=Language.PYTHON, \n",
    "        parser_threshold=500\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_loader1 = loader.load()\n",
    "document_loader2 = loader2.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap = 400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chunks = document_splitter.split_documents(document_loader1)\n",
    "document_chunks2 = document_splitter.split_documents(document_loader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import os\\nimport zipfile\\nimport gdown\\nfrom src.cnnClassifier.utils.common import get_size\\nfrom src.cnnClassifier.entity.config_entity import (DataIngestionConfig)\\nfrom src.cnnClassifier.constants import constants\\n\\nfrom src.logger import logging\\n\\nlogger = logging.getLogger(\"DataIngestion\")\\n\\nclass DataIngestion:\\n    def __init__(self, config: DataIngestionConfig):\\n        self.config = config\\n\\n    \\n    def download_file(self)-> str:\\n        \\'\\'\\'\\n        Fetch data from the url\\n        \\'\\'\\'\\n\\n        try: \\n            dataset_url = self.config.source_URL\\n            zip_download_dir = self.config.local_data_file\\n            os.makedirs(self.config.unzip_dir, exist_ok=True)\\n            logger.info(f\"Downloading data from {dataset_url} into file {zip_download_dir}\")\\n\\n            file_id = dataset_url.split(\"/\")[-2]\\n            prefix = constants.GOOGLE_DRIVE_DOWNLOAD_PREFIX_URL\\n            gdown.download(prefix+file_id,zip_download_dir)\\n\\n            logger.info(f\"Downloaded data from {dataset_url} into file {zip_download_dir}\")\\n\\n        except Exception as e:\\n            raise e\\n        \\n    \\n\\n    def extract_zip_file(self):\\n        \"\"\"\\n        zip_file_path: str\\n        Extracts the zip file into the data directory\\n        Function returns None\\n        \"\"\"\\n        unzip_path = self.config.unzip_dir\\n        os.makedirs(unzip_path, exist_ok=True)\\n        with zipfile.ZipFile(self.config.local_data_file, \\'r\\') as zip_ref:\\n            zip_ref.extractall(unzip_path)\\n        os.remove(self.config.local_data_file)', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport zipfile\\nimport gdown\\nfrom src.cnnClassifier.utils.common import *\\nfrom src.cnnClassifier.entity.config_entity import PrepareBaseModelConfig\\nfrom src.cnnClassifier.constants import constants\\n\\nimport tensorflow as tf\\n\\nfrom src.logger import logging\\n\\nlogger = logging.getLogger(\"BaseModelPrepration\")', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\components\\\\prepare_base_model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='class PrepareBaseModel:\\n    def __init__(self, config: PrepareBaseModelConfig):\\n        self.config = config\\n    \\n    def get_base_model(self):\\n        # self.model = tf.keras.applications.vgg16.VGG16(\\n        #     weights=self.config.params_weights,\\n        #     input_shape=self.config.parmas_image_size,\\n        #     include_top=self.config.params_include_top\\n        # )\\n\\n        self.model = tf.keras.applications.efficientnet.EfficientNetB0(\\n            weights=self.config.params_weights,\\n            input_shape=self.config.parmas_image_size,\\n            include_top=self.config.params_include_top\\n        )\\n\\n        self.save_model(\\n            path = self.config.base_model_path,\\n            model = self.model\\n        )\\n\\n    @staticmethod\\n    def _prepare_full_model(model, classes, freeze_all, freeze_till, learning_rate):\\n        if freeze_all:\\n            for layer in model.layers:\\n                model.trainable = False\\n        elif (freeze_till is not None) and (freeze_till > 0):\\n            for layer in model.layers[:-freeze_till]:\\n                model.trainable = False\\n\\n        preprocessed_input = tf.keras.applications.efficientnet.preprocess_input(\\n            tf.keras.layers.Input(shape=(224, 224, 3))\\n        )\\n        efficient_X = model(preprocessed_input)\\n        efficient_X = tf.keras.layers.GlobalAveragePooling2D()(efficient_X)\\n        efficient_X = tf.keras.layers.Dropout(0.2)(efficient_X)\\n        efficient_X = tf.keras.layers.Dense(\\n            units=512,\\n            activation=\"relu\"\\n        )(efficient_X)\\n        efficient_X = tf.keras.layers.Dropout(0.2)(efficient_X)\\n        efficient_X = tf.keras.layers.Dense(\\n            units = classes,\\n            activation=\"softmax\"\\n        )(efficient_X)\\n\\n        full_model = tf.keras.models.Model(\\n            inputs=preprocessed_input,\\n            outputs=efficient_X\\n        )', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\components\\\\prepare_base_model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='full_model = tf.keras.models.Model(\\n            inputs=preprocessed_input,\\n            outputs=efficient_X\\n        )\\n\\n        full_model.compile(\\n            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\\n            loss=tf.keras.losses.CategoricalCrossentropy(),\\n            metrics=[\"accuracy\"]\\n        )\\n\\n        full_model.summary()\\n        return full_model\\n    \\n    \\n    def update_base_model(self):\\n        self.full_model = self._prepare_full_model(\\n            model=self.model,\\n            classes=self.config.params_classes,\\n            freeze_all=True,\\n            freeze_till=None,\\n            learning_rate=self.config.params_learning_rate\\n        )\\n\\n        self.save_model(path=self.config.updated_base_model_path, model=self.full_model)\\n\\n    @staticmethod\\n    def save_model(path: Path, model: tf.keras.Model):\\n        model.save(path)', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\components\\\\prepare_base_model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport tensorflow as tf\\nimport time\\nfrom pathlib import Path\\nimport sys\\n\\nfrom cnnClassifier.entity.config_entity import TrainingConfig\\nfrom src.logger import logging\\nfrom src.exception import CustomException\\n\\nlogger = logging.getLogger(\"ModelTrainingClass\")', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\components\\\\training_model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='class Training:\\n    def __init__(self, config: TrainingConfig):\\n        self.config = config\\n\\n    \\n    def get_base_model(self):\\n        self.model = tf.keras.models.load_model(\\n            self.config.updated_base_model_path\\n        )\\n\\n    def train_valid_generator(self):\\n\\n        datagenerator_kwargs = dict(\\n            rescale = 1./255,\\n            validation_split=0.20\\n        )\\n\\n        dataflow_kwargs = dict(\\n            target_size=self.config.params_image_size[:-1],\\n            batch_size=self.config.params_batch_size,\\n            interpolation=\"bilinear\"\\n        )\\n\\n        valid_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\\n            **datagenerator_kwargs\\n        )\\n\\n        self.valid_generator = valid_datagenerator.flow_from_directory(\\n            directory=self.config.training_data,\\n            subset=\"validation\",\\n            shuffle=False,\\n            **dataflow_kwargs\\n        )\\n\\n        if self.config.params_is_augmentation:\\n            train_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\\n                rotation_range=40,\\n                horizontal_flip=True,\\n                width_shift_range=0.2,\\n                height_shift_range=0.2,\\n                shear_range=0.2,\\n                zoom_range=0.2,\\n                **datagenerator_kwargs\\n            )\\n        else:\\n            train_datagenerator = valid_datagenerator\\n\\n        self.train_generator = train_datagenerator.flow_from_directory(\\n            directory=self.config.training_data,\\n            subset=\"training\",\\n            shuffle=True,\\n            **dataflow_kwargs\\n        )', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\components\\\\training_model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='self.train_generator = train_datagenerator.flow_from_directory(\\n            directory=self.config.training_data,\\n            subset=\"training\",\\n            shuffle=True,\\n            **dataflow_kwargs\\n        )\\n\\n    def get_train_validation_data(self):\\n        logger.info(\"Entering into get_train_validation_data()......\")\\n        try:\\n            self.train_generator = tf.keras.utils.image_dataset_from_directory(\\n                directory = self.config.training_data,\\n                labels = \"inferred\",\\n                label_mode=\"categorical\",\\n                shuffle = True,\\n                class_names = self.config.params_classes,\\n                seed = 42,\\n                image_size=tuple(self.config.params_image_size[0:2]),\\n                batch_size=self.config.params_batch_size\\n            )\\n        except Exception as exe:\\n            logger.exception(exe)\\n            raise CustomException(exe, sys)\\n\\n        try:\\n            self.valid_generator = tf.keras.utils.image_dataset_from_directory(\\n                directory = self.config.validation_data,\\n                labels=\"inferred\",\\n                label_mode=\"categorical\",\\n                class_names= self.config.params_classes,\\n                seed=42,\\n                image_size=tuple(self.config.params_image_size[0:2]),\\n                batch_size=self.config.params_batch_size\\n            )\\n        except Exception as exe:\\n            logger.exception(exe)\\n            raise CustomException(exe, sys)\\n\\n    @staticmethod\\n    def save_model(path: Path, model: tf.keras.Model):\\n        model.save(path)\\n\\n\\n    def model_training(self):\\n        logger.info(\"Entering into model_training().........\")', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\components\\\\training_model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='@staticmethod\\n    def save_model(path: Path, model: tf.keras.Model):\\n        model.save(path)\\n\\n\\n    def model_training(self):\\n        logger.info(\"Entering into model_training().........\")\\n\\n        try:\\n            self.model.fit(\\n                self.train_generator,\\n                epochs = self.config.params_epochs,\\n                validation_data = self.valid_generator,\\n                verbose = 1\\n            )\\n            self.save_model(\\n                path=self.config.trained_model_path,\\n                model = self.model\\n            )\\n        except Exception as exe:\\n            logger.exception(exe)\\n            raise CustomException(exe, sys)\\n    \\n    def train(self):\\n        self.steps_per_epoch = self.train_generator.samples // self.train_generator.batch_size\\n        self.validation_steps = self.valid_generator.samples // self.valid_generator.batch_size\\n\\n        self.model.fit(\\n            self.train_generator,\\n            epochs=self.config.params_epochs,\\n            steps_per_epoch=self.steps_per_epoch,\\n            validation_steps=self.validation_steps,\\n            validation_data=self.valid_generator\\n        )\\n\\n        self.save_model(\\n            path=self.config.trained_model_path,\\n            model=self.model\\n        )', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\components\\\\training_model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from pathlib import Path\\nimport os\\nimport sys\\n\\nfrom src.cnnClassifier.constants import constants\\nfrom src.cnnClassifier.utils.common import read_yaml, create_directories\\nfrom src.cnnClassifier.entity.config_entity import (DataIngestionConfig, PrepareBaseModelConfig, TrainingConfig)\\nfrom src.logger import logging\\nfrom src.exception import CustomException\\n\\nlogger = logging.getLogger(\"ConfigurationManager\")', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\config\\\\configuration.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='class ConfigurationManager:\\n    def __init__(\\n        self,\\n        config_filepath = constants.CONFIG_FILE_PATH,\\n        params_filepath = constants.PARAMS_FILE_PATH):\\n\\n        self.config = read_yaml(config_filepath)\\n        self.params = read_yaml(params_filepath)\\n\\n        create_directories([self.config.artifacts_root])\\n\\n\\n    \\n    def get_data_ingestion_config(self) -> DataIngestionConfig:\\n        logger.info(\"Entering into get_data_ingestion_config()........\")\\n        config = self.config.data_ingestion\\n\\n        create_directories([config.root_dir])\\n        try:\\n            data_ingestion_config = DataIngestionConfig(\\n                root_dir=config.root_dir,\\n                source_URL=config.source_URL,\\n                local_data_file=config.local_data_file,\\n                unzip_dir=config.unzip_dir \\n            )\\n        except Exception as exe:\\n            logger.exception(exe)\\n            raise CustomException(exe, sys)\\n\\n        return data_ingestion_config\\n    \\n    def prepare_base_model(self) -> PrepareBaseModelConfig:\\n        logger.info(\"Entering into prepare_base_model().......\")\\n        config = self.config.prepare_base_model', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\config\\\\configuration.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='return data_ingestion_config\\n    \\n    def prepare_base_model(self) -> PrepareBaseModelConfig:\\n        logger.info(\"Entering into prepare_base_model().......\")\\n        config = self.config.prepare_base_model\\n\\n        try:\\n            prepare_base_model_config = PrepareBaseModelConfig(\\n                root_dir=Path(config.root_dir),\\n                base_model_path=Path(config.base_model_path),\\n                updated_base_model_path=Path(config.updated_base_model_path),\\n                params_include_top=self.params.INCLUDE_TOP,\\n                params_learning_rate=self.params.LEARNING_RATE,\\n                params_weights=self.params.WEIGHTS,\\n                params_classes=self.params.CLASSES,\\n                parmas_image_size=self.params.IMAGE_SIZE\\n            )\\n        except Exception as exe:\\n            logger.exception(exe)\\n            raise CustomException(exe, sys)\\n        \\n        return prepare_base_model_config \\n    \\n    def get_training_config(self) -> TrainingConfig:\\n        logger.info(\"Entering into get_training_config().........\")\\n        training = self.config.training\\n        prepare_base_model = self.config.prepare_base_model\\n        params = self.params\\n        training_data = os.path.join(self.config.data_ingestion.unzip_dir, constants.IMAGE_DATA_FOLDER_NAME, \"train\")\\n        validation_data = os.path.join(self.config.data_ingestion.unzip_dir, constants.IMAGE_DATA_FOLDER_NAME, \"valid\")\\n        create_directories([\\n            Path(training.root_dir)\\n        ])\\n        total_classes = os.listdir(os.path.join(self.config.data_ingestion.unzip_dir, constants.IMAGE_DATA_FOLDER_NAME, \"train\"))', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\config\\\\configuration.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='try:\\n            training_config = TrainingConfig(\\n                root_dir=Path(training.root_dir),\\n                trained_model_path=Path(training.trained_model_path),\\n                updated_base_model_path=Path(prepare_base_model.updated_base_model_path),\\n                training_data=Path(training_data),\\n                validation_data=Path(validation_data),\\n                params_epochs=params.EPOCHS,\\n                params_batch_size=params.BATCH_SIZE,\\n                params_is_augmentation=params.AUGMENTATION,\\n                params_image_size=params.IMAGE_SIZE,\\n                params_total_classes = len(total_classes),\\n                params_classes=total_classes\\n            )\\n        except Exception as exe:\\n            logger.exception(exe)\\n            raise CustomException(exe, sys)\\n\\n        return training_config', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\config\\\\configuration.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from pathlib import Path\\n\\nCONFIG_FILE_PATH = Path(\"config/config.yaml\")\\nPARAMS_FILE_PATH = Path(\"params.yaml\")\\nGOOGLE_DRIVE_DOWNLOAD_PREFIX_URL = \"https://drive.google.com/uc?/export=download&id=\"\\n\\nIMAGE_DATA_FOLDER_NAME = \"common_data\"\\n\\nUNZIP_DIR = \"artifacts/data_ingestion\"', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\constants\\\\constants.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from dataclasses import dataclass\\nfrom pathlib import Path\\n\\n@dataclass(frozen=True)\\nclass DataIngestionConfig:\\n    root_dir: Path\\n    source_URL: str\\n    local_data_file: Path\\n    unzip_dir: Path\\n\\n\\n@dataclass(frozen=True)\\nclass PrepareBaseModelConfig:\\n    root_dir: Path\\n    base_model_path: Path\\n    updated_base_model_path: Path\\n    parmas_image_size: list\\n    params_learning_rate: float\\n    params_include_top: bool\\n    params_weights: str\\n    params_classes: int\\n    \\n\\n@dataclass(frozen=True)\\nclass TrainingConfig:\\n    root_dir: Path\\n    trained_model_path: Path\\n    updated_base_model_path: Path\\n    training_data: Path\\n    validation_data: Path\\n    params_epochs: int\\n    params_batch_size: int\\n    params_is_augmentation: bool\\n    params_image_size: list\\n    params_total_classes: int\\n    params_classes: list', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from src.cnnClassifier.config.configuration import ConfigurationManager\\nfrom src.cnnClassifier.components.data_ingestion import DataIngestion\\nfrom logger import logging\\n\\nSTAGE_NAME = \"Data Ingestion stage\"\\n\\nlogger = logging.getLogger(\"DataIngestionPipeline\")\\n\\nclass DataIngestionTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        data_ingestion_config = config.get_data_ingestion_config()\\n        data_ingestion = DataIngestion(config=data_ingestion_config)\\n        data_ingestion.download_file()\\n        data_ingestion.extract_zip_file()\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = DataIngestionTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\pipeline\\\\stage_01_data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from src.cnnClassifier.config.configuration import ConfigurationManager\\nfrom src.cnnClassifier.components.prepare_base_model import PrepareBaseModel\\n\\nfrom logger import logging\\n\\nlogger = logging.getLogger(\"BaseModelCreationPipeline\")\\nSTAGE_NAME = \"Prepare Base Model stage\"\\n\\nclass PrepareBaseModelPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        prepare_base_model_config = config.prepare_base_model()\\n        base_model = PrepareBaseModel(config=prepare_base_model_config)\\n        base_model.get_base_model()\\n        base_model.update_base_model()\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\"*******************\")\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = PrepareBaseModelPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\pipeline\\\\stage_02_prepare_base_model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from src.cnnClassifier.config.configuration import ConfigurationManager\\nfrom src.cnnClassifier.components.training_model import Training\\n\\nimport sys\\n\\nfrom logger import logging\\nfrom exception import CustomException\\n\\nlogger = logging.getLogger(\"ModelTraining\")\\nSTAGE_NAME = \"Model Training\"\\n\\nclass ModelTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        training_config = config.get_training_config()\\n        training = Training(config=training_config)\\n        training.get_base_model()\\n        training.get_train_validation_data()\\n        training.model_training()\\n\\n\\n\\nif __name__ == \"__main__\":\\n    try:\\n        logger.info(f\"*******************\")\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = ModelTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise CustomException(e, sys)', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\pipeline\\\\stage_03_model_training.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport tensorflow as tf\\nimport numpy as np\\nimport sys\\n\\nfrom src.exception import CustomException\\nfrom src.logger import logging\\nfrom src.cnnClassifier.entity.config_entity import TrainingConfig\\nfrom src.cnnClassifier.constants import constants\\n\\nlogger = logging.getLogger(\"Prediction Pipeline\")\\n\\nclass PredictionPipeline:\\n    def __init__(self, fileName):\\n        self.fileName = fileName\\n        \\n    \\n    def predict_image(self):\\n        logger.info(\"Entering into predict_image()............\")\\n        classes = []\\n        result = None\\n        try:\\n            model = tf.keras.models.load_model(os.path.join(\"model\", \"model.h5\"))\\n        \\n            logger.info(\"FileName: {}\".format(self.fileName))\\n            test_image = tf.keras.preprocessing.image.load_img(\\n                self.fileName, \\n                target_size = (224, 224)\\n            )\\n\\n            test_image = tf.keras.preprocessing.image.img_to_array(test_image)\\n            test_image = np.expand_dims(test_image, axis=0)\\n            \\n            result = np.argmax(\\n                model.predict(test_image),\\n                axis=1\\n            )\\n            logger.info(\"After prediction result is {}\".format(result))\\n            classes = os.listdir(os.path.join(constants.UNZIP_DIR, constants.IMAGE_DATA_FOLDER_NAME, \"train\"))\\n            \\n            logger.info(\"Actual Name: {}\".format(classes[result[0]]))\\n        except Exception as e:\\n            logger.exception(e)\\n            raise CustomException(e, sys)\\n        \\n        return [{\\n            \"image\": classes[result[0]]\\n        }]', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\pipeline\\\\stage_04_prediction.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\nimport yaml\\nimport json\\nimport joblib\\nfrom ensure import ensure_annotations\\nfrom box import ConfigBox\\nfrom pathlib import Path\\nfrom typing import Any\\nimport base64\\n\\n\\n\\nfrom src.logger import logging\\nfrom src.cnnClassifier.constants import constants\\nfrom src.exception import CustomException\\n\\nlogger = logging.getLogger(\"CommonFile\")\\n\\n@ensure_annotations\\ndef read_yaml(path_to_yaml: Path) -> ConfigBox:\\n    \"\"\"reads yaml file and returns\\n\\n    Args:\\n        path_to_yaml (str): path like input\\n\\n    Raises:\\n        ValueError: if yaml file is empty\\n        e: empty file\\n\\n    Returns:\\n        ConfigBox: ConfigBox type\\n    \"\"\"\\n    try:\\n        with open(path_to_yaml) as yaml_file:\\n            content = yaml.safe_load(yaml_file)\\n            logger.info(f\"yaml file: {path_to_yaml} loaded successfully\")\\n            return ConfigBox(content)\\n    except Exception as e:\\n        raise CustomException(e, sys)\\n    \\n\\n\\n@ensure_annotations\\ndef create_directories(path_to_directories: list, verbose=True):\\n    \"\"\"create list of directories\\n\\n    Args:\\n        path_to_directories (list): list of path of directories\\n        ignore_log (bool, optional): ignore if multiple dirs is to be created. Defaults to False.\\n    \"\"\"\\n    for path in path_to_directories:\\n        os.makedirs(path, exist_ok=True)\\n        if verbose:\\n            logger.info(f\"created directory at: {path}\")\\n\\n\\n@ensure_annotations\\ndef save_json(path: Path, data: dict):\\n    \"\"\"save json data\\n\\n    Args:\\n        path (Path): path to json file\\n        data (dict): data to be saved in json file\\n    \"\"\"\\n    with open(path, \"w\") as f:\\n        json.dump(data, f, indent=4)\\n\\n    logger.info(f\"json file saved at: {path}\")\\n\\n\\n\\n\\n@ensure_annotations', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\utils\\\\common.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='def save_json(path: Path, data: dict):\\n    \"\"\"save json data\\n\\n    Args:\\n        path (Path): path to json file\\n        data (dict): data to be saved in json file\\n    \"\"\"\\n    with open(path, \"w\") as f:\\n        json.dump(data, f, indent=4)\\n\\n    logger.info(f\"json file saved at: {path}\")\\n\\n\\n\\n\\n@ensure_annotations\\ndef load_json(path: Path) -> ConfigBox:\\n    \"\"\"load json files data\\n\\n    Args:\\n        path (Path): path to json file\\n\\n    Returns:\\n        ConfigBox: data as class attributes instead of dict\\n    \"\"\"\\n    with open(path) as f:\\n        content = json.load(f)\\n\\n    logger.info(f\"json file loaded succesfully from: {path}\")\\n    return ConfigBox(content)\\n\\n\\n@ensure_annotations\\ndef save_bin(data: Any, path: Path):\\n    \"\"\"save binary file\\n\\n    Args:\\n        data (Any): data to be saved as binary\\n        path (Path): path to binary file\\n    \"\"\"\\n    joblib.dump(value=data, filename=path)\\n    logger.info(f\"binary file saved at: {path}\")\\n\\n\\n@ensure_annotations\\ndef load_bin(path: Path) -> Any:\\n    \"\"\"load binary data\\n\\n    Args:\\n        path (Path): path to binary file\\n\\n    Returns:\\n        Any: object stored in the file\\n    \"\"\"\\n    data = joblib.load(path)\\n    logger.info(f\"binary file loaded from: {path}\")\\n    return data\\n\\n@ensure_annotations\\ndef get_size(path: Path) -> str:\\n    \"\"\"get size in KB\\n\\n    Args:\\n        path (Path): path of the file\\n\\n    Returns:\\n        str: size in KB\\n    \"\"\"\\n    size_in_kb = round(os.path.getsize(path)/1024)\\n    return f\"~ {size_in_kb} KB\"\\n\\n\\ndef decodeImage(imgstring, fileName):\\n    imgdata = base64.b64decode(imgstring)\\n    with open(fileName, \\'wb\\') as f:\\n        f.write(imgdata)\\n        f.close()\\n\\n\\ndef encodeImageIntoBase64(croppedImagePath):\\n    with open(croppedImagePath, \"rb\") as f:\\n        return base64.b64encode(f.read())', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\utils\\\\common.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='from flask import Flask, request, jsonify, render_template\\nimport os\\nfrom flask_cors import CORS, cross_origin\\n\\nfrom src.cnnClassifier.utils.common import decodeImage\\nfrom src.cnnClassifier.pipeline.stage_04_prediction import PredictionPipeline\\n\\nfrom src.logger import logging\\nfrom src.exception import CustomException\\n\\nlogger = logging.getLogger(\"App\")\\n\\nos.putenv(\\'LANG\\', \\'en_US.UTF-8\\')\\nos.putenv(\\'LC_ALL\\', \\'en_US.UTF-8\\')\\n\\napp = Flask(__name__)\\nCORS(app)\\n\\nclass ClientApp:\\n    def __init__(self):\\n        self.fileName = \"inputImage.jpg\"\\n        self.classifier = PredictionPipeline(self.fileName)\\n\\n\\n@app.route(\"/\", methods=[\\'GET\\'])\\n@cross_origin()\\ndef home():\\n    return render_template(\\'index.html\\')\\n\\n\\n@app.route(\"/train\", methods=[\\'GET\\',\\'POST\\'])\\n@cross_origin()\\ndef trainRoute():\\n    os.system(\"python main.py\")\\n    # os.system(\"dvc repro\")\\n    return \"Training done successfully!\"\\n\\n\\n@app.route(\"/predict\", methods=[\\'POST\\'])\\n@cross_origin()\\ndef predictRoute():\\n    image = request.json[\\'image\\']\\n    decodeImage(image, clApp.fileName)\\n    result = clApp.classifier.predict_image()\\n    return jsonify(result)\\n\\n\\nif __name__ == \"__main__\":\\n    clApp = ClientApp()\\n    app.run(host=\\'0.0.0.0\\', port=8080)', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import sys\\n\\nfrom src.logger import logging\\nfrom src.exception import CustomException\\nfrom src.cnnClassifier.pipeline import (stage_01_data_ingestion, stage_02_prepare_base_model, stage_03_model_training)\\n\\nlogger = logging.getLogger(\"MainFile\")\\n\\nSTAGE_NAME = \"Data Ingestion stage\"\\ntry:\\n   logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\") \\n   data_ingestion = stage_01_data_ingestion.DataIngestionTrainingPipeline()\\n   data_ingestion.main()\\n   logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\nexcept Exception as e:\\n   logger.exception(e)\\n   raise CustomException(e, sys)\\n\\nSTAGE_NAME = \"Prepare Base Model stage\"\\ntry:\\n   logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\") \\n   prepare_base_model = stage_02_prepare_base_model.PrepareBaseModelPipeline()\\n   prepare_base_model.main()\\n   logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\nexcept Exception as exe:\\n   logger.exception(exe)\\n   raise CustomException(exe, sys)\\n\\nSTAGE_NAME = \"Model Training\"\\ntry:\\n   logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\") \\n   model_training = stage_03_model_training.ModelTrainingPipeline()\\n   model_training.main()\\n   logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\nexcept Exception as e:\\n   logger.exception(e)\\n   raise CustomException(e, sys)', metadata={'source': 'test_repo\\\\main.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import setuptools\\n\\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as f:\\n    long_description = f.read()\\n\\n\\n__version__ = \"0.0.1\"\\n\\nREPO_NAME = \"Sport_Classification_Using_Images_Project\"\\nAUTHOR_USER_NAME = \"Manav446\"\\nSRC_REPO = \"cnnClassifier\"\\nAUTHOR_EMAIL = \"manavgarg026@gmail.com\"\\n\\n\\nsetuptools.setup(\\n    name=SRC_REPO,\\n    version=__version__,\\n    author=AUTHOR_USER_NAME,\\n    author_email=AUTHOR_EMAIL,\\n    description=\"A small python package for CNN app\",\\n    long_description=long_description,\\n    long_description_content=\"text/markdown\",\\n    url=f\"https://github.com/{AUTHOR_USER_NAME}/{REPO_NAME}\",\\n    project_urls={\\n        \"Bug Tracker\": f\"https://github.com/{AUTHOR_USER_NAME}/{REPO_NAME}/issues\",\\n    },\\n    package_dir={\"\": \"src\"},\\n    packages=setuptools.find_packages(where=\"src\")\\n)', metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\n#logging string\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\nproject_name = \\'cnnClassifier\\'\\n\\nlist_of_files = [\\n    \".github/workflows/.gitkeep\",\\n    f\"src/{project_name}/__init__.py\",\\n    f\"src/{project_name}/components/__init__.py\",\\n    f\"src/{project_name}/utils/__init__.py\",\\n    f\"src/{project_name}/config/__init__.py\",\\n    f\"src/{project_name}/config/configuration.py\",\\n    f\"src/{project_name}/pipeline/__init__.py\",\\n    f\"src/{project_name}/entity/__init__.py\",\\n    f\"src/{project_name}/constants/__init__.py\",\\n    \"config/config.yaml\",\\n    \"dvc.yaml\",\\n    \"params.yaml\",\\n    \"requirements.txt\",\\n    \"setup.py\",\\n    \"research/trials.ipynb\",\\n    \"templates/index.html\"\\n\\n\\n]\\n\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n\\n\\n    if filedir !=\"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n\\n\\n    else:\\n        logging.info(f\"{filename} is already exists\")', metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import sys\\n\\ndef error_message_details(error, error_detail:sys):\\n    _,_,exc_tb = error_detail.exc_info()\\n    \\n    error_message = \"error occured in python script name [{0}] line number [{1}] error message [{2}]\".format(\\n        exc_tb.tb_frame.f_code.co_filename,\\n        exc_tb.tb_lineno,\\n        str(error)\\n    )\\n    return error_message\\n\\nclass CustomException(Exception):\\n    def __init__(self, error_message, error_detail):\\n        super().__init__(error_message)\\n        self.error_message = error_message_details(error_message, error_detail)\\n\\n    def __str__(self):\\n        return self.error_message', metadata={'source': 'test_repo\\\\src\\\\exception.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import logging\\nimport os\\nimport sys\\nfrom datetime import datetime\\n\\nLOG_FILE = \"running_logs.log\"\\nLOG_PATH = os.path.join(os.getcwd(), \"logs\")\\nos.makedirs(LOG_PATH, exist_ok=True)\\n\\nLOG_FILE_PATH = os.path.join(LOG_PATH, LOG_FILE)\\n\\nlogging.basicConfig(\\n    # format = \"[ %(asctime)s ] %(filename)s:%(lineno)d %(name)s - %(levelname)s - %(message)s\",\\n    format = \"[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s - %(message)s\",\\n    level=logging.INFO,\\n    handlers=[\\n        logging.FileHandler(LOG_FILE_PATH),\\n        logging.StreamHandler(sys.stdout),\\n    ]\\n)\\n\\n#logger = logging.getLogger(\"cnnClassifierLogger\")', metadata={'source': 'test_repo\\\\src\\\\logger.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport zipfile\\nimport gdown\\nfrom src.cnnClassifier.utils.common import get_size\\nfrom src.cnnClassifier.entity.config_entity import (DataIngestionConfig)\\nfrom src.cnnClassifier.constants import constants\\n\\nfrom src.logger import logging\\n\\nlogger = logging.getLogger(\"DataIngestion\")\\n\\nclass DataIngestion:\\n    def __init__(self, config: DataIngestionConfig):\\n        self.config = config\\n\\n    \\n    def download_file(self)-> str:\\n        \\'\\'\\'\\n        Fetch data from the url\\n        \\'\\'\\'\\n\\n        try: \\n            dataset_url = self.config.source_URL\\n            zip_download_dir = self.config.local_data_file\\n            os.makedirs(self.config.unzip_dir, exist_ok=True)\\n            logger.info(f\"Downloading data from {dataset_url} into file {zip_download_dir}\")\\n\\n            file_id = dataset_url.split(\"/\")[-2]\\n            prefix = constants.GOOGLE_DRIVE_DOWNLOAD_PREFIX_URL\\n            gdown.download(prefix+file_id,zip_download_dir)\\n\\n            logger.info(f\"Downloaded data from {dataset_url} into file {zip_download_dir}\")\\n\\n        except Exception as e:\\n            raise e\\n        \\n    \\n\\n    def extract_zip_file(self):\\n        \"\"\"\\n        zip_file_path: str\\n        Extracts the zip file into the data directory\\n        Function returns None\\n        \"\"\"\\n        unzip_path = self.config.unzip_dir\\n        os.makedirs(unzip_path, exist_ok=True)\\n        with zipfile.ZipFile(self.config.local_data_file, \\'r\\') as zip_ref:\\n            zip_ref.extractall(unzip_path)\\n        os.remove(self.config.local_data_file)', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport zipfile\\nimport gdown\\nfrom src.cnnClassifier.utils.common import *\\nfrom src.cnnClassifier.entity.config_entity import PrepareBaseModelConfig\\nfrom src.cnnClassifier.constants import constants\\n\\nimport tensorflow as tf\\n\\nfrom src.logger import logging\\n\\nlogger = logging.getLogger(\"BaseModelPrepration\")', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\components\\\\prepare_base_model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='class PrepareBaseModel:\\n    def __init__(self, config: PrepareBaseModelConfig):\\n        self.config = config\\n    \\n    def get_base_model(self):\\n        # self.model = tf.keras.applications.vgg16.VGG16(\\n        #     weights=self.config.params_weights,\\n        #     input_shape=self.config.parmas_image_size,\\n        #     include_top=self.config.params_include_top\\n        # )\\n\\n        self.model = tf.keras.applications.efficientnet.EfficientNetB0(\\n            weights=self.config.params_weights,\\n            input_shape=self.config.parmas_image_size,\\n            include_top=self.config.params_include_top\\n        )\\n\\n        self.save_model(\\n            path = self.config.base_model_path,\\n            model = self.model\\n        )\\n\\n    @staticmethod\\n    def _prepare_full_model(model, classes, freeze_all, freeze_till, learning_rate):\\n        if freeze_all:\\n            for layer in model.layers:\\n                model.trainable = False\\n        elif (freeze_till is not None) and (freeze_till > 0):\\n            for layer in model.layers[:-freeze_till]:\\n                model.trainable = False\\n\\n        preprocessed_input = tf.keras.applications.efficientnet.preprocess_input(\\n            tf.keras.layers.Input(shape=(224, 224, 3))\\n        )\\n        efficient_X = model(preprocessed_input)\\n        efficient_X = tf.keras.layers.GlobalAveragePooling2D()(efficient_X)\\n        efficient_X = tf.keras.layers.Dropout(0.2)(efficient_X)\\n        efficient_X = tf.keras.layers.Dense(\\n            units=512,\\n            activation=\"relu\"\\n        )(efficient_X)\\n        efficient_X = tf.keras.layers.Dropout(0.2)(efficient_X)\\n        efficient_X = tf.keras.layers.Dense(\\n            units = classes,\\n            activation=\"softmax\"\\n        )(efficient_X)\\n\\n        full_model = tf.keras.models.Model(\\n            inputs=preprocessed_input,\\n            outputs=efficient_X\\n        )', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\components\\\\prepare_base_model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='full_model = tf.keras.models.Model(\\n            inputs=preprocessed_input,\\n            outputs=efficient_X\\n        )\\n\\n        full_model.compile(\\n            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\\n            loss=tf.keras.losses.CategoricalCrossentropy(),\\n            metrics=[\"accuracy\"]\\n        )\\n\\n        full_model.summary()\\n        return full_model\\n    \\n    \\n    def update_base_model(self):\\n        self.full_model = self._prepare_full_model(\\n            model=self.model,\\n            classes=self.config.params_classes,\\n            freeze_all=True,\\n            freeze_till=None,\\n            learning_rate=self.config.params_learning_rate\\n        )\\n\\n        self.save_model(path=self.config.updated_base_model_path, model=self.full_model)\\n\\n    @staticmethod\\n    def save_model(path: Path, model: tf.keras.Model):\\n        model.save(path)', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\components\\\\prepare_base_model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport tensorflow as tf\\nimport time\\nfrom pathlib import Path\\nimport sys\\n\\nfrom cnnClassifier.entity.config_entity import TrainingConfig\\nfrom src.logger import logging\\nfrom src.exception import CustomException\\n\\nlogger = logging.getLogger(\"ModelTrainingClass\")', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\components\\\\training_model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='class Training:\\n    def __init__(self, config: TrainingConfig):\\n        self.config = config\\n\\n    \\n    def get_base_model(self):\\n        self.model = tf.keras.models.load_model(\\n            self.config.updated_base_model_path\\n        )\\n\\n    def train_valid_generator(self):\\n\\n        datagenerator_kwargs = dict(\\n            rescale = 1./255,\\n            validation_split=0.20\\n        )\\n\\n        dataflow_kwargs = dict(\\n            target_size=self.config.params_image_size[:-1],\\n            batch_size=self.config.params_batch_size,\\n            interpolation=\"bilinear\"\\n        )\\n\\n        valid_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\\n            **datagenerator_kwargs\\n        )\\n\\n        self.valid_generator = valid_datagenerator.flow_from_directory(\\n            directory=self.config.training_data,\\n            subset=\"validation\",\\n            shuffle=False,\\n            **dataflow_kwargs\\n        )\\n\\n        if self.config.params_is_augmentation:\\n            train_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\\n                rotation_range=40,\\n                horizontal_flip=True,\\n                width_shift_range=0.2,\\n                height_shift_range=0.2,\\n                shear_range=0.2,\\n                zoom_range=0.2,\\n                **datagenerator_kwargs\\n            )\\n        else:\\n            train_datagenerator = valid_datagenerator\\n\\n        self.train_generator = train_datagenerator.flow_from_directory(\\n            directory=self.config.training_data,\\n            subset=\"training\",\\n            shuffle=True,\\n            **dataflow_kwargs\\n        )', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\components\\\\training_model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='self.train_generator = train_datagenerator.flow_from_directory(\\n            directory=self.config.training_data,\\n            subset=\"training\",\\n            shuffle=True,\\n            **dataflow_kwargs\\n        )\\n\\n    def get_train_validation_data(self):\\n        logger.info(\"Entering into get_train_validation_data()......\")\\n        try:\\n            self.train_generator = tf.keras.utils.image_dataset_from_directory(\\n                directory = self.config.training_data,\\n                labels = \"inferred\",\\n                label_mode=\"categorical\",\\n                shuffle = True,\\n                class_names = self.config.params_classes,\\n                seed = 42,\\n                image_size=tuple(self.config.params_image_size[0:2]),\\n                batch_size=self.config.params_batch_size\\n            )\\n        except Exception as exe:\\n            logger.exception(exe)\\n            raise CustomException(exe, sys)\\n\\n        try:\\n            self.valid_generator = tf.keras.utils.image_dataset_from_directory(\\n                directory = self.config.validation_data,\\n                labels=\"inferred\",\\n                label_mode=\"categorical\",\\n                class_names= self.config.params_classes,\\n                seed=42,\\n                image_size=tuple(self.config.params_image_size[0:2]),\\n                batch_size=self.config.params_batch_size\\n            )\\n        except Exception as exe:\\n            logger.exception(exe)\\n            raise CustomException(exe, sys)\\n\\n    @staticmethod\\n    def save_model(path: Path, model: tf.keras.Model):\\n        model.save(path)\\n\\n\\n    def model_training(self):\\n        logger.info(\"Entering into model_training().........\")', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\components\\\\training_model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='@staticmethod\\n    def save_model(path: Path, model: tf.keras.Model):\\n        model.save(path)\\n\\n\\n    def model_training(self):\\n        logger.info(\"Entering into model_training().........\")\\n\\n        try:\\n            self.model.fit(\\n                self.train_generator,\\n                epochs = self.config.params_epochs,\\n                validation_data = self.valid_generator,\\n                verbose = 1\\n            )\\n            self.save_model(\\n                path=self.config.trained_model_path,\\n                model = self.model\\n            )\\n        except Exception as exe:\\n            logger.exception(exe)\\n            raise CustomException(exe, sys)\\n    \\n    def train(self):\\n        self.steps_per_epoch = self.train_generator.samples // self.train_generator.batch_size\\n        self.validation_steps = self.valid_generator.samples // self.valid_generator.batch_size\\n\\n        self.model.fit(\\n            self.train_generator,\\n            epochs=self.config.params_epochs,\\n            steps_per_epoch=self.steps_per_epoch,\\n            validation_steps=self.validation_steps,\\n            validation_data=self.valid_generator\\n        )\\n\\n        self.save_model(\\n            path=self.config.trained_model_path,\\n            model=self.model\\n        )', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\components\\\\training_model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from pathlib import Path\\nimport os\\nimport sys\\n\\nfrom src.cnnClassifier.constants import constants\\nfrom src.cnnClassifier.utils.common import read_yaml, create_directories\\nfrom src.cnnClassifier.entity.config_entity import (DataIngestionConfig, PrepareBaseModelConfig, TrainingConfig)\\nfrom src.logger import logging\\nfrom src.exception import CustomException\\n\\nlogger = logging.getLogger(\"ConfigurationManager\")', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\config\\\\configuration.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='class ConfigurationManager:\\n    def __init__(\\n        self,\\n        config_filepath = constants.CONFIG_FILE_PATH,\\n        params_filepath = constants.PARAMS_FILE_PATH):\\n\\n        self.config = read_yaml(config_filepath)\\n        self.params = read_yaml(params_filepath)\\n\\n        create_directories([self.config.artifacts_root])\\n\\n\\n    \\n    def get_data_ingestion_config(self) -> DataIngestionConfig:\\n        logger.info(\"Entering into get_data_ingestion_config()........\")\\n        config = self.config.data_ingestion\\n\\n        create_directories([config.root_dir])\\n        try:\\n            data_ingestion_config = DataIngestionConfig(\\n                root_dir=config.root_dir,\\n                source_URL=config.source_URL,\\n                local_data_file=config.local_data_file,\\n                unzip_dir=config.unzip_dir \\n            )\\n        except Exception as exe:\\n            logger.exception(exe)\\n            raise CustomException(exe, sys)\\n\\n        return data_ingestion_config\\n    \\n    def prepare_base_model(self) -> PrepareBaseModelConfig:\\n        logger.info(\"Entering into prepare_base_model().......\")\\n        config = self.config.prepare_base_model', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\config\\\\configuration.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='return data_ingestion_config\\n    \\n    def prepare_base_model(self) -> PrepareBaseModelConfig:\\n        logger.info(\"Entering into prepare_base_model().......\")\\n        config = self.config.prepare_base_model\\n\\n        try:\\n            prepare_base_model_config = PrepareBaseModelConfig(\\n                root_dir=Path(config.root_dir),\\n                base_model_path=Path(config.base_model_path),\\n                updated_base_model_path=Path(config.updated_base_model_path),\\n                params_include_top=self.params.INCLUDE_TOP,\\n                params_learning_rate=self.params.LEARNING_RATE,\\n                params_weights=self.params.WEIGHTS,\\n                params_classes=self.params.CLASSES,\\n                parmas_image_size=self.params.IMAGE_SIZE\\n            )\\n        except Exception as exe:\\n            logger.exception(exe)\\n            raise CustomException(exe, sys)\\n        \\n        return prepare_base_model_config \\n    \\n    def get_training_config(self) -> TrainingConfig:\\n        logger.info(\"Entering into get_training_config().........\")\\n        training = self.config.training\\n        prepare_base_model = self.config.prepare_base_model\\n        params = self.params\\n        training_data = os.path.join(self.config.data_ingestion.unzip_dir, constants.IMAGE_DATA_FOLDER_NAME, \"train\")\\n        validation_data = os.path.join(self.config.data_ingestion.unzip_dir, constants.IMAGE_DATA_FOLDER_NAME, \"valid\")\\n        create_directories([\\n            Path(training.root_dir)\\n        ])\\n        total_classes = os.listdir(os.path.join(self.config.data_ingestion.unzip_dir, constants.IMAGE_DATA_FOLDER_NAME, \"train\"))', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\config\\\\configuration.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='try:\\n            training_config = TrainingConfig(\\n                root_dir=Path(training.root_dir),\\n                trained_model_path=Path(training.trained_model_path),\\n                updated_base_model_path=Path(prepare_base_model.updated_base_model_path),\\n                training_data=Path(training_data),\\n                validation_data=Path(validation_data),\\n                params_epochs=params.EPOCHS,\\n                params_batch_size=params.BATCH_SIZE,\\n                params_is_augmentation=params.AUGMENTATION,\\n                params_image_size=params.IMAGE_SIZE,\\n                params_total_classes = len(total_classes),\\n                params_classes=total_classes\\n            )\\n        except Exception as exe:\\n            logger.exception(exe)\\n            raise CustomException(exe, sys)\\n\\n        return training_config', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\config\\\\configuration.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from pathlib import Path\\n\\nCONFIG_FILE_PATH = Path(\"config/config.yaml\")\\nPARAMS_FILE_PATH = Path(\"params.yaml\")\\nGOOGLE_DRIVE_DOWNLOAD_PREFIX_URL = \"https://drive.google.com/uc?/export=download&id=\"\\n\\nIMAGE_DATA_FOLDER_NAME = \"common_data\"\\n\\nUNZIP_DIR = \"artifacts/data_ingestion\"', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\constants\\\\constants.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from dataclasses import dataclass\\nfrom pathlib import Path\\n\\n@dataclass(frozen=True)\\nclass DataIngestionConfig:\\n    root_dir: Path\\n    source_URL: str\\n    local_data_file: Path\\n    unzip_dir: Path\\n\\n\\n@dataclass(frozen=True)\\nclass PrepareBaseModelConfig:\\n    root_dir: Path\\n    base_model_path: Path\\n    updated_base_model_path: Path\\n    parmas_image_size: list\\n    params_learning_rate: float\\n    params_include_top: bool\\n    params_weights: str\\n    params_classes: int\\n    \\n\\n@dataclass(frozen=True)\\nclass TrainingConfig:\\n    root_dir: Path\\n    trained_model_path: Path\\n    updated_base_model_path: Path\\n    training_data: Path\\n    validation_data: Path\\n    params_epochs: int\\n    params_batch_size: int\\n    params_is_augmentation: bool\\n    params_image_size: list\\n    params_total_classes: int\\n    params_classes: list', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from src.cnnClassifier.config.configuration import ConfigurationManager\\nfrom src.cnnClassifier.components.data_ingestion import DataIngestion\\nfrom logger import logging\\n\\nSTAGE_NAME = \"Data Ingestion stage\"\\n\\nlogger = logging.getLogger(\"DataIngestionPipeline\")\\n\\nclass DataIngestionTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        data_ingestion_config = config.get_data_ingestion_config()\\n        data_ingestion = DataIngestion(config=data_ingestion_config)\\n        data_ingestion.download_file()\\n        data_ingestion.extract_zip_file()\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = DataIngestionTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\pipeline\\\\stage_01_data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from src.cnnClassifier.config.configuration import ConfigurationManager\\nfrom src.cnnClassifier.components.prepare_base_model import PrepareBaseModel\\n\\nfrom logger import logging\\n\\nlogger = logging.getLogger(\"BaseModelCreationPipeline\")\\nSTAGE_NAME = \"Prepare Base Model stage\"\\n\\nclass PrepareBaseModelPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        prepare_base_model_config = config.prepare_base_model()\\n        base_model = PrepareBaseModel(config=prepare_base_model_config)\\n        base_model.get_base_model()\\n        base_model.update_base_model()\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\"*******************\")\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = PrepareBaseModelPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\pipeline\\\\stage_02_prepare_base_model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from src.cnnClassifier.config.configuration import ConfigurationManager\\nfrom src.cnnClassifier.components.training_model import Training\\n\\nimport sys\\n\\nfrom logger import logging\\nfrom exception import CustomException\\n\\nlogger = logging.getLogger(\"ModelTraining\")\\nSTAGE_NAME = \"Model Training\"\\n\\nclass ModelTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        training_config = config.get_training_config()\\n        training = Training(config=training_config)\\n        training.get_base_model()\\n        training.get_train_validation_data()\\n        training.model_training()\\n\\n\\n\\nif __name__ == \"__main__\":\\n    try:\\n        logger.info(f\"*******************\")\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = ModelTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise CustomException(e, sys)', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\pipeline\\\\stage_03_model_training.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport tensorflow as tf\\nimport numpy as np\\nimport sys\\n\\nfrom src.exception import CustomException\\nfrom src.logger import logging\\nfrom src.cnnClassifier.entity.config_entity import TrainingConfig\\nfrom src.cnnClassifier.constants import constants\\n\\nlogger = logging.getLogger(\"Prediction Pipeline\")\\n\\nclass PredictionPipeline:\\n    def __init__(self, fileName):\\n        self.fileName = fileName\\n        \\n    \\n    def predict_image(self):\\n        logger.info(\"Entering into predict_image()............\")\\n        classes = []\\n        result = None\\n        try:\\n            model = tf.keras.models.load_model(os.path.join(\"model\", \"model.h5\"))\\n        \\n            logger.info(\"FileName: {}\".format(self.fileName))\\n            test_image = tf.keras.preprocessing.image.load_img(\\n                self.fileName, \\n                target_size = (224, 224)\\n            )\\n\\n            test_image = tf.keras.preprocessing.image.img_to_array(test_image)\\n            test_image = np.expand_dims(test_image, axis=0)\\n            \\n            result = np.argmax(\\n                model.predict(test_image),\\n                axis=1\\n            )\\n            logger.info(\"After prediction result is {}\".format(result))\\n            classes = os.listdir(os.path.join(constants.UNZIP_DIR, constants.IMAGE_DATA_FOLDER_NAME, \"train\"))\\n            \\n            logger.info(\"Actual Name: {}\".format(classes[result[0]]))\\n        except Exception as e:\\n            logger.exception(e)\\n            raise CustomException(e, sys)\\n        \\n        return [{\\n            \"image\": classes[result[0]]\\n        }]', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\pipeline\\\\stage_04_prediction.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\nimport yaml\\nimport json\\nimport joblib\\nfrom ensure import ensure_annotations\\nfrom box import ConfigBox\\nfrom pathlib import Path\\nfrom typing import Any\\nimport base64\\n\\n\\n\\nfrom src.logger import logging\\nfrom src.cnnClassifier.constants import constants\\nfrom src.exception import CustomException\\n\\nlogger = logging.getLogger(\"CommonFile\")\\n\\n@ensure_annotations\\ndef read_yaml(path_to_yaml: Path) -> ConfigBox:\\n    \"\"\"reads yaml file and returns\\n\\n    Args:\\n        path_to_yaml (str): path like input\\n\\n    Raises:\\n        ValueError: if yaml file is empty\\n        e: empty file\\n\\n    Returns:\\n        ConfigBox: ConfigBox type\\n    \"\"\"\\n    try:\\n        with open(path_to_yaml) as yaml_file:\\n            content = yaml.safe_load(yaml_file)\\n            logger.info(f\"yaml file: {path_to_yaml} loaded successfully\")\\n            return ConfigBox(content)\\n    except Exception as e:\\n        raise CustomException(e, sys)\\n    \\n\\n\\n@ensure_annotations\\ndef create_directories(path_to_directories: list, verbose=True):\\n    \"\"\"create list of directories\\n\\n    Args:\\n        path_to_directories (list): list of path of directories\\n        ignore_log (bool, optional): ignore if multiple dirs is to be created. Defaults to False.\\n    \"\"\"\\n    for path in path_to_directories:\\n        os.makedirs(path, exist_ok=True)\\n        if verbose:\\n            logger.info(f\"created directory at: {path}\")\\n\\n\\n@ensure_annotations\\ndef save_json(path: Path, data: dict):\\n    \"\"\"save json data\\n\\n    Args:\\n        path (Path): path to json file\\n        data (dict): data to be saved in json file\\n    \"\"\"\\n    with open(path, \"w\") as f:\\n        json.dump(data, f, indent=4)\\n\\n    logger.info(f\"json file saved at: {path}\")\\n\\n\\n\\n\\n@ensure_annotations', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\utils\\\\common.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='def save_json(path: Path, data: dict):\\n    \"\"\"save json data\\n\\n    Args:\\n        path (Path): path to json file\\n        data (dict): data to be saved in json file\\n    \"\"\"\\n    with open(path, \"w\") as f:\\n        json.dump(data, f, indent=4)\\n\\n    logger.info(f\"json file saved at: {path}\")\\n\\n\\n\\n\\n@ensure_annotations\\ndef load_json(path: Path) -> ConfigBox:\\n    \"\"\"load json files data\\n\\n    Args:\\n        path (Path): path to json file\\n\\n    Returns:\\n        ConfigBox: data as class attributes instead of dict\\n    \"\"\"\\n    with open(path) as f:\\n        content = json.load(f)\\n\\n    logger.info(f\"json file loaded succesfully from: {path}\")\\n    return ConfigBox(content)\\n\\n\\n@ensure_annotations\\ndef save_bin(data: Any, path: Path):\\n    \"\"\"save binary file\\n\\n    Args:\\n        data (Any): data to be saved as binary\\n        path (Path): path to binary file\\n    \"\"\"\\n    joblib.dump(value=data, filename=path)\\n    logger.info(f\"binary file saved at: {path}\")\\n\\n\\n@ensure_annotations\\ndef load_bin(path: Path) -> Any:\\n    \"\"\"load binary data\\n\\n    Args:\\n        path (Path): path to binary file\\n\\n    Returns:\\n        Any: object stored in the file\\n    \"\"\"\\n    data = joblib.load(path)\\n    logger.info(f\"binary file loaded from: {path}\")\\n    return data\\n\\n@ensure_annotations\\ndef get_size(path: Path) -> str:\\n    \"\"\"get size in KB\\n\\n    Args:\\n        path (Path): path of the file\\n\\n    Returns:\\n        str: size in KB\\n    \"\"\"\\n    size_in_kb = round(os.path.getsize(path)/1024)\\n    return f\"~ {size_in_kb} KB\"\\n\\n\\ndef decodeImage(imgstring, fileName):\\n    imgdata = base64.b64decode(imgstring)\\n    with open(fileName, \\'wb\\') as f:\\n        f.write(imgdata)\\n        f.close()\\n\\n\\ndef encodeImageIntoBase64(croppedImagePath):\\n    with open(croppedImagePath, \"rb\") as f:\\n        return base64.b64encode(f.read())', metadata={'source': 'test_repo\\\\src\\\\cnnClassifier\\\\utils\\\\common.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chunks2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 26)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_chunks), len(document_chunks2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Chroma Vector DB for saving document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db = Chroma.from_documents(\n",
    "    document_chunks2, \n",
    "    embedding=gemini_embeddings, \n",
    "    persist_directory=\"./data\"\n",
    ")\n",
    "\n",
    "chroma_db.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Google GEMINI LLM MODEL Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = ChatGoogleGenerativeAI(model = \"models/gemini-1.5-pro\", temperature=0.7, top_p=0.8, google_api_key=GEMINI_API_KEY, convert_system_message_to_human=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading memory object for LLM chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "momory_obj = ConversationSummaryMemory(llm=llm_model, \n",
    "                                       memory_key=\"chat_history\", \n",
    "                                       return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm_model, \n",
    "    retriever=chroma_db.as_retriever(\n",
    "        search_type=\"mmr\", search_kwargs={\"k\": 3}\n",
    "        ), \n",
    "    memory=momory_obj\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question and Anwering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"what is a DataIngestion class?\"\n",
    "\n",
    "response = conversation_chain.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what is a DataIngestion class?',\n",
       " 'chat_history': [SystemMessage(content='')],\n",
       " 'answer': \"This code doesn't contain a `DataIngestion` class. It defines a `DataIngestionConfig` dataclass, which holds configuration information for data ingestion, but there's no class named `DataIngestion` present. \\n\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] template=\"You are an powerful assistantfor question-answering tasks You have to tell the Usr.\\nUse the following context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\nUse five sentences maximum and keep the answer concise.\\n\\nQuestion: {question} \\nContext: {context} \\nAnswer:\\n\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm_prompt_template = \"\"\"You are an powerful assistantfor question-answering tasks You have to tell the Usr.\n",
    "Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use five sentences maximum and keep the answer concise.\\n\n",
    "Question: {question} \\nContext: {context} \\nAnswer:\n",
    "\"\"\"\n",
    "\n",
    "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
    "\n",
    "print(llm_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.prompt_template import format_document\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": chroma_db.as_retriever(\n",
    "        search_type=\"mmr\", search_kwargs={\"k\": 3}\n",
    "    ) | format_docs, \"question\": RunnablePassthrough() }\n",
    "    | llm_prompt \n",
    "    | llm_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_2 = rag_chain.invoke(input=\"What does a config_entity file contains?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A config_entity file seems to define classes related to configurations, such as `DataIngestionConfig`, `PrepareBaseModelConfig`, and `TrainingConfig`. These classes likely hold parameters and settings for different stages of a machine learning pipeline.  The provided code snippet shows how instances of these classes are created and populated with data from YAML configuration files.  However, the exact contents of a config_entity file are not shown in the provided code. \\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
